<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distributed processing and HPC environments · COBREXA.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="COBREXA.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="COBREXA.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">COBREXA.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../">Tutorials</a></li><li><a class="tocitem" href="../../notebooks/">Examples and notebooks</a></li><li><a class="tocitem" href="../../functions/">Function reference</a></li><li><a class="tocitem" href="../../howToContribute/">How to contribute</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Distributed processing and HPC environments</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distributed processing and HPC environments</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/LCSB-BioCore/COBREXA.jl/blob/master/docs/src/tutorials/3_hpc.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Distributed-processing-and-HPC-environments"><a class="docs-heading-anchor" href="#Distributed-processing-and-HPC-environments">Distributed processing and HPC environments</a><a id="Distributed-processing-and-HPC-environments-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-processing-and-HPC-environments" title="Permalink"></a></h1><p>Distributed processing in Julia is represented mainly by the package <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/"><code>Distributed.jl</code></a>.</p><p><code>COBREXA.jl</code> is able to utilize this existing system to almost transparently run the large parallelizable analyses on multiple CPU cores and multiple computers connected through the network. Ultimately, the approach scales to thousands of computing nodes in large HPC facilities.</p><p>Here, we give a short overview of how to work in the distributed environment and utilize the resources for <code>COBREXA.jl</code> analyses.</p><h2 id="Starting-the-distributed-workers"><a class="docs-heading-anchor" href="#Starting-the-distributed-workers">Starting the distributed workers</a><a id="Starting-the-distributed-workers-1"></a><a class="docs-heading-anchor-permalink" href="#Starting-the-distributed-workers" title="Permalink"></a></h2><p><code>COBREXA.jl</code> follows the structure imposed by the <code>Distributed</code> package: You operate a main (usually called &quot;master&quot;) computation node, connect to multiple other computers and start worker Julia processes there, and distribute the workload across this cluster.</p><p>To start, you need to load the package and add a few processes. This starts 5 processes locally:</p><pre><code class="language-none">using Distributed
addprocs(5)</code></pre><div class="admonition is-info"><header class="admonition-header">`Distributed.jl` installation</header><div class="admonition-body"><p><code>Distributed.jl</code> usually comes pre-installed with Julia distribution, but you may still need to &quot;enable&quot; it by typing <code>] add Distributed</code>.</p></div></div><p>You may check that the workers are really there, using <code>workers()</code>. In this case, it should give you a vector of <em>worker IDs</em>, very likely equal to <code>[2,3,4,5,6]</code>.</p><p>If you have compute resources available via a network, you may connect these as well, provided you have a secure shell (<code>ssh</code>) access to them. You will likely want to establish a key-based authentication (refer to <a href="https://www.openssh.com/manual.html">ssh documentation</a>) to make the connection easier.</p><p>With shell, check that you can <code>ssh</code> to a remote node and run Julia there:</p><pre><code class="language-none">user@pc&gt; ssh server
...
user@server&gt; julia
...
julia&gt; _</code></pre><div class="admonition is-category-top"><header class="admonition-header">Running shell commands from Julia</header><div class="admonition-body"><p>If you don&#39;t want to quit your Julia session to try out the <code>ssh</code> connection from the shell, press <code>;</code> in the Julia prompt on the beginning of the line. The interpreter will execute your next line as a shell command.</p></div></div><p>If this works for you, you can add some workers that run on the <code>server</code> from your Julia shell running on your <code>pc</code>. For example, the following starts 20 workers on the remote <code>server</code> and 10 workers on your friend&#39;s computer called <code>joe_pc</code>:</p><pre><code class="language-none">addprocs([(&#39;server&#39;, 20), (&#39;joe_pc&#39;, 10)])</code></pre><p>With this, you can schedule various computation on the workers; see the Julia manual of <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/"><code>Distributed</code></a> for basic details. You may try various convenience packages, such as <a href="https://github.com/JuliaParallel/DistributedArrays.jl"><code>DistributedArrays.jl</code></a> and <a href="https://github.com/LCSB-BioCore/DistributedData.jl"><code>DistributedData.jl</code></a>, to process any data in a distributed fashion.</p><h2 id="Running-a-distributed-analysis"><a class="docs-heading-anchor" href="#Running-a-distributed-analysis">Running a distributed analysis</a><a id="Running-a-distributed-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Running-a-distributed-analysis" title="Permalink"></a></h2><p>While not all COBREXA functions may be parallelized naturally, these that do will accept a special <code>workers</code> argument that specifies a list of worker IDs where the computation should be distributed. For the value, you can specify your desired worker IDs manually (e.g. <code>[2,3,4]</code>), or simply use <code>workers()</code>.</p><p>For example, <a href="../../functions/#COBREXA.flux_variability_analysis-Tuple{MetabolicModel, Any}"><code>flux_variability_analysis</code></a> can naturally parallelize the computation of all reactions&#39;s minima and maxima to finish the computation faster. To enable the parallelization, you first need to make sure that all workers have loaded both the COBREXA package and the optimizer:</p><pre><code class="language-none">using COBREXA, GLPK, Distributed
addprocs(10)                       # add any kind and number of processes here
@everywhere using COBREXA, GLPK    # loads the necessary packages on all workers</code></pre><p>When the package is loaded and precompiled everywhere, you may load your model and run the FVA with the <code>workers</code> parameter:</p><pre><code class="language-none">model = load_model(&quot;e_coli_core.xml&quot;)
result = flux_variability_analysis(model, GLPK.Optimizer; workers=workers())</code></pre><p>With the extra computing capacity from <code>N</code> workers available, the FVA should be computed roughly <code>N</code>-times faster.</p><div class="admonition is-info"><header class="admonition-header">Distribution and parallelization overhead</header><div class="admonition-body"><p>Communication of the workers with your Julia shell is not free. If the task that you are parallelizing is small and the model structure is very large, the distributed computation will actually spend most computation time just distributing the large model to the workers, and almost no time in executing the small parallel task. In such case, the performance will not improve by adding additional resources. You may want to check that the computation task is sufficiently large before investing the extra resources into the distributed execution. <a href="https://en.wikipedia.org/wiki/Amdahl&#39;s_law">Amdahl&#39;s</a> and <a href="https://en.wikipedia.org/wiki/Gustafson%27s_law">Gustafson&#39;s</a> laws can give you a better overview of the consequences of this overhead.</p></div></div><h2 id="Interacting-with-HPC-schedulers"><a class="docs-heading-anchor" href="#Interacting-with-HPC-schedulers">Interacting with HPC schedulers</a><a id="Interacting-with-HPC-schedulers-1"></a><a class="docs-heading-anchor-permalink" href="#Interacting-with-HPC-schedulers" title="Permalink"></a></h2><p>Many researchers have access to institutional HPC facilities that allow time-sharing of the capacity of a large computer cluster between many researchers. Julia and <code>COBREXA.jl</code> work well within this environment; but your programs require some additional customization to be able to find and utilize the resources available from the HPC.</p><p>In our case, this reduces to a relatively complex task: You need to find out how many resources were allocated for your task, and you need to add the remote workers precisely at places that were allocated for your. Fortunately, the package <a href="https://github.com/JuliaParallel/ClusterManagers.jl"><code>ClusterManagers.jl</code></a> can do precisely that.</p><p>For simplicily, we will assume that your HPC is scheduled by <a href="https://slurm.schedmd.com/">Slurm</a>.</p><p>Adding of the workers from Slurm is done as follows:</p><ul><li>you import the <code>ClusterManagers</code> package</li><li>you find how many processes to spawn from the environment from <code>SLURM_NTASKS</code> environment variable</li><li>you use the function <code>addprocs_slurm</code> to precisely connect to your allocated computational resources</li></ul><p>The Julia script that does a parallel analysis may then start as follows:</p><pre><code class="language-none">using COBREXA, Distributed, ClusterManagers

available_workers = parse(Int, ENV[&quot;SLURM_NTASKS&quot;])

addprocs_slurm(available_workers)

...
result = flux_variability_analysis(...; workers=workers())
...</code></pre><p>After adding the Slurm workers, you may continue as if the workers were added using normal <code>addprocs</code>, and (for example) run the <a href="../../functions/#COBREXA.flux_variability_analysis-Tuple{MetabolicModel, Any}"><code>flux_variability_analysis</code></a> as shown above.</p><div class="admonition is-success"><header class="admonition-header">What about the other HPC schedulers?</header><div class="admonition-body"><p><code>ClusterManagers.jl</code> supports many other common HPC scheduling systems, including LFS, Sun Grid, SGE, PBS, and Scyld, in a way almost identical to Slurm. See the <a href="https://github.com/JuliaParallel/ClusterManagers.jl/blob/master/README.md">package documentation</a> for details.</p></div></div><h2 id="Wrapping-your-script-in-a-Slurm-job"><a class="docs-heading-anchor" href="#Wrapping-your-script-in-a-Slurm-job">Wrapping your script in a Slurm job</a><a id="Wrapping-your-script-in-a-Slurm-job-1"></a><a class="docs-heading-anchor-permalink" href="#Wrapping-your-script-in-a-Slurm-job" title="Permalink"></a></h2><p>To be able to submit your script for later processing using the <a href="https://slurm.schedmd.com/sbatch.html"><code>sbatch</code> Slurm command</a>, you need to wrap it in a small &quot;batch&quot; script that tells Slurm how many resources the process needs.</p><p>Assuming you have a Julia computation script written down in <code>myJob.jl</code> and saved on your HPC cluster&#39;s access node, the corresponding Slurm batch script (let&#39;s call it <code>myJob.sbatch</code>) may look as follows:</p><pre><code class="language-none">#!/bin/bash -l
#SBATCH -n 100           # the job will require 100 individual workers
#SBATCH -c 1             # each worker will sit on a single CPU
#SBATCH -t 30            # the whole job will take less than 30 minutes
#SBATCH -J myJob         # the name of the job

module load lang/Julia   # this is usually required to make Julia available to your job

julia myJob.jl</code></pre><p>To run the computation, simply run <code>sbatch myJob.sbatch</code> on the access node. The job will be scheduled and eventually executed. You may watch <code>sacct</code> and <code>squeue</code> in the meantime, to see the progress.</p><p>Remember that you need to explicitly save the result of your Julia script computation to files, to be able to retrieve them later. Standard outputs of the jobs are often mangled and discarded. If you still want to collect the standard output, you may change the last line of the batch script to</p><pre><code class="language-none">julia myJob.jl &gt; myJob.log</code></pre><p>and collect the output from the log later. This is convenient especially if logging various computation details using the <code>@info</code> and similar macros.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 11 May 2021 10:44">Tuesday 11 May 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
